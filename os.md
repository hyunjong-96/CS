# 운영체제(OS)



<br>

-----------------------

### 운영체제란

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 컴퓨터의 하드웨어와 소프트웨어가 통신하고 작동할수 있도록하는 소프트웨어 환경
- 목적
  - 컴퓨터의 계산 활동을 관리하여 컴퓨터 시스템이 제대로 작동하도록 한다.
  - 프로그램의 개발 및 실행을 위한 환경 제공.

</details>

-----------------------

<br>



<br>

-----------------------

### 프로세스와 스레드

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 프로세스 : 메모리에 올라와서 실행되는 프로그램의 실행 단위
- 스레드 : 프로세스 안에서 역할을 수행하는 흐름 단위, 다른 스레드와 자원 공유 가능
- 스레드는 메모리에서 stack만 할당 받으며, 프로세스의 code, data, heap영역의 메모리를 공유한다.

</details>

-----------------------

<br>



<br>

-----------------------

### 멀티프로세스와 멀티스레드

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 멀티 프로세스
  - 하나의 프로그램이 여러개의 프로세스로 구성되어 프로그램을 병렬적으로 작업 수행
  - 멀티 프로세스는 하나의 프로세스에 문제가 발생하면 다른 프로세스에 영향을 미치지 않지만 멀티 스레드보다 많은 메모리를 필요로 한다.
- 멀티 스레드
  - 하나의 프로세스에서 여러개의 스레드로 구성되어 자원을 공유하며 작업을 수행
  - 멀티 프로세스보다 적은 메모리를 필요로 하지만 하나의 스레드에 문제가 발생하면 다른 스레드에 영향을 미쳐 프로그램이 종료될수 있다.
  - 멀티스레드가 절대적으로 좋은것은 아니다.
    - 멀티 스레드는 메모리를 공유하기 때문에 멀티스레드를 많이 사용하는 경우 하나의 스레드에 문제가 발생해도 다른 스레드에 문제가 발생할 수 있다.
    - 빈번한 context switching이 발생하기 때문에 성능이 저하된다.

</details>

-----------------------

<br>



<br>

-----------------------

### 멀티 프로세스보다 멀티 스레드를 사용하는 이유

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 멀티 스레드는 멀티 프로세스보다 적은 메모리 공간을 차지하고 Context Switching이 빠른 장점이 있다.
- 그리고 공유 메모리를 사용하기 때문에 통신을 위한 system call이 줄어 통신 비용이 줄고 프로세스 생성에 필요한 system call이 줄어 자원을 효율적으로 관리할 수 있다.
- 하지만 동기화 문제나 하나의 스레드 장애로 전체 스레드가 종료될 수 있는 단점이 있다.
- 컨텍스트 스위칭시 stack영역만 초기화 하면 된다.

</details>

-----------------------

<br>



<br>

-----------------------

### 스레드마다 stack을 독립적으로 할당하는 이유

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- stack은 함수 호출시 전달되는 인자, 지역변수, 돌아갈 주소값 등을 저장하기 위한 메모리 공간
- stack을 독립적으로 할당한다는 것은 함수를 독립적으로 실행하는 것이 가능해진다. 이는 독립적인 실행 흐름을 추가하는것이다.
- 즉, 작업을 실행하면서 스레드로 독립적인 실행흐름을 추가하기 위해 스레드에 stack을 독립적으로 할당하는것이다.

</details>

-----------------------

<br>



<br>

-----------------------

### 스레드마다 PC Register를 독립적으로 할당하는 이유

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 스레드의 pc register 에는 명령을 어디까지 수행했는지 저장하는 메모리 공간
- context switching이 발생할 때, 명령 수행 위치를 저장해놔야 다음 cpu에 할당되었을때 명령을 이어서 수행하기 위해 pc register를 독립적으로 할당한다.

</details>

-----------------------

<br>



<br>

-----------------------

### 자바 스레드

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 일반 스레드와 동일하게 프로그램의 흐름 단위.
- JVM에 의해 관리되며 JVM은 프로세스가 없고 스레드만 존재한다.
- 스레드가 생성되면 스레드 정보를 Thread Control Block에 저장하고 JVM에 의해 관리된다.
  - TCB : 커널의 데이터 구조
    - id
    - 스레드 상태
    - 스레드 레지스터 값
    - 스레드가 있는 프로세스의 PCB에 대한 포인터


</details>

-----------------------

<br>



<br>

-----------------------

### 하드웨어 스레드 vs 소프트웨어 스레드

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />




-----------------------

- <img width="678" alt="image" src="https://user-images.githubusercontent.com/57162257/214998943-37a17e89-522c-4b3c-8335-c1525f12a352.png">
- 하드웨어 스레드
  - CPU의 작업을 병렬적으로 처리하는 코어에서 처리할수 있는 최소 단위
- 소프트웨어 스레드
  - 프로그램 내부에서 생성된 스레드
  - 생성은 여러개 될 수 있지만 코어의 하드웨어 스레드개수만큼만 실행이 가능, 나머지 소프트웨어 스레드는 대기 상태
  - 자바에서 스레드풀을 통해 100개의 스레드를 만들 수는 있지만, 모두 사용될 수 있는 스레드는 아니다. 하드웨어 스레드 개수만큼 100개의 스레드 중 실행이 된다.

- 4코어 8스레드라는 것은 하나의 책이 4부작으로 나뉘어져있고 4부작의 책은 총 8명의 사람만이 동시에 볼 수 있다.


</details>

-----------------------

<br>



<br>

-----------------------

### 고아 프로세스 와 좀비 프로세스

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />




-----------------------

- 프로세스는 커널 모드로 fork()를 통해 자식 프로세스를 생성할 수 있습니다.

  - 부모 프로세스와 자식 프로세스가 서로 다른 작업을 수행하기 위해서는 fork()후 exec()를 통해 별도의 프로세스로 덮어씌어야합니다.

- `좀비 프로세스`

  - 프로세스가 종료되게 되면, 프로세스에서 사용하던 자원을 운영체제에 반환하게 된다. 그리고 부모 프로세스는 종료된 자식 프로세스의 상태를 회수하기 위해 wait()으로 자식 프로세스 종료를 기다리고 자원을 반납한다.

    wait을 통해서 자식 프로세스의 종료 상태를 회수함으로써 프로세스 테이블을 삭제한다.

  - 사용한 자원을 운영체제에 반환된 프로세스를 좀비프로세스라고한다.

  - 프로세스가 생성되게 된다면, 프로세스 테이블에 프로세스의 PID와 종료상태를 가지고 있습니다. 만약 부모 프로세스가 자식 프로세스의 상태를 회수(wait())하지 못한다면, 프로세스 테이블에 프로세스 정보가 유지되게 되고 다른 프로세스가 자원을 사용하는데 문제가 발생할 수 있습니다.

- 자식 프로세스가 작업이 끝나지 않았는데, 부모 프로세스가 종료되어버리면 자식 프로세스는 `고아 프로세스`가 됩니다.

  - 자식 프로세스가 고아 프로세스가 되어버리면, init 프로세스(PID : 1)이 고아 프로세스의 부모 프로세스가 됩니다. 그리고 고아 프로세스가 종료되면 init 프로세스에서 고아 프로세스의 종료 상태를 회수함으로써 좀비 프로세스를 방지합니다.


init 프로세스

- init 프로세스는 운영체제에서 부팅시 생성되는 최초의 프로세스로써 시스템이 종료될때까지 유지되는 데몬 프로세스입니다.
- PID 1

데몬 프로세스

- 백그라운드 프로세스 중 PPID(부모프로세스)가 1인 프로세스로써, 사용자와 직접 소통하지 않고 백그라운드에서 다른 프로세스의 작업을 뒷받침하는 프로세스


</details>

-----------------------

<br>



<br>

-----------------------

### 메모리(RAM)

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

<img width="30%" alt="image" src="https://user-images.githubusercontent.com/57162257/182611233-ad4eacd7-2f5b-4957-8146-8e481339fc67.png">

- 메모리 : 프로그램에서 필요로하고 생성되는 값을 저장하는 저장 공간
- 코드 영역 : 실행한 프로그램의 코드 저장 영역
- 데이터 영역 : 전역 변수, 정적 변수 저장 영역
- 힙 영역  : 개발자의 동적으로 할당되는 데이터 저장 영역, 런타임시 크기가 결정된다.
  - 장점:
    - 필요한 데이터의 크기를 알 수 없을 때 사용가능
  - 단점
    - 크기가 동적이고 메모리할당 크기 등 여러 요소를 고려하기 때문에 접근 속도가 느리고 CPU계산 비용이 추가로 필요하다.
    - 힙 경합시 속도 저하 (여러 스레드가 힙 영역의 데이터를 동시에 접근하는 문제)
- 스택 영역 : 지역변수, 매개변수 저장 영역, 컴파일시 크기가 결정된다.
  - 장점
    - 낭비되는 공간이 없다(단편화 x)
    - 크기가 고정되어 있어 포인터 이동만 하면되기 때문에 접근 속도가 빠르고 CPU계산 비용이 적다
  - 단점
    - 공간을 유연하게 사용할 수 없다.
  - 스택 오버 플로우
    - <img width="500" alt="image" src="https://user-images.githubusercontent.com/57162257/189783086-9f266128-ed46-4155-9253-30b6b1c32faf.png">
    - 재귀 호출이 반복되면 스택 프레임이 계속 쌓이고 스택에 모든 공간을 차지하게 되면 발생되는 에러



</details>

-----------------------

<br>



<br>

-----------------------

### Stack 과 Heap

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />




-----------------------

- Stack
  - 장점
    - 메모리의 낭비가 적다(단편화 x)
    - 순차적으로 실행되고 크기가 고정되어있기 때문에 데이터 접근이 빠르고 CPU연산 비용이 적다

  - 단점
    - 크기가 정해져있기 때문에 메모리를 유연하게 사용할수 없다.

- Heap
  - 장점
    - 런타임 시점에 크기가 결정되기 때문에 프로그램을 실행하는데 어떤 개체가 얼만큼 필요한지 모르는 경우 사용하기 좋다.
    - 주소만 알고있다면 순서에 상관없이 데이터를 참조할 수 있다.

  - 단점
    - 메모리 크기가 동적으로 할당되기 때문에 메모리의 비효율이 발생할 수 있다.(단편화)
    - 크기가 동적이기 때문에 데이터 접근이 오래걸리고 CPU연산 비용이 추가로 발생한다.
    - 힙 경합시 속도 저하

- Stack과 Heap을 분리한 이유
  - 메모리를 사용할때, 메모리의 크기를 미리 선언하여 메모리 비효율을 줄일 수 있고(stack), 프로그램에 필요한 개체가 어떤것이 필요하고 얼만큼 필요한지 모르기 때문에 유연하게 사용하기 위함(heap)


</details>

-----------------------

<br>



<br>

-----------------------

### 기억장치

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 데이터, 프로그램, 연산 중간 결과등을 일시적 또는 영구적으로 저장하는 장치
- 레지스터 : CPU에 있는 작지만 빠른 기억장치
- 캐시 : CPU 주기억장치의 접근속도 차이를 해결하기 위한 저장소
- 주 기억장치 : CPU가 직접 읽고 쓸수 있는 장치 (RAM, ROM), 레지스터에 비해 속도가 느리다.
- 보조 기억장치 : CPU가 직접 일고 쓸수 없지만 대용량인 저장소

</details>

-----------------------

<br>



<br>

-----------------------

### 가상 메모리

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 프로세스 전체가 메모리에 올라오지 않고 실행에 필요한 부분만 메모리에 올려서 프로그램을 실행 시킬수 있는 기법
- 가상 메모리가 없다면 프로세스 전체를 메모리에 올려서 사용해야하기 때문에 메모리의 크기에 제약이 많이 따른다.
- 가상 메모리를 사용하게 되면 프로세스에서 필요한 부분만 메모리에 올려서 프로그램을 실행할 수 있기 때문에 메모리의 크기에 제약을 받지 않는다.
- CPU이용률과 처리량이 높아졌다.
- 페이징을 통해 필요한 프로세스를 메모리에 올린다.
- 하지만 물리 메모리로 프로세스를 구동시키는 것보다 느리다.

</details>

-----------------------

<br>



<br>

-----------------------

### 요구 페이징

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- CPU가 프로세스를 실행시킬때 필요한 데이터(페이지)를 메모리에 적재하는 전략
- 가상 메모리는 페이징으로 관리된다.
- 페이지 부재(page fault) 발생시 mmu(memory management unit)에 의해서 인터럽트를 발생시켜 프로세스를 wait상태로 변경 한 후, 요구하는 페이지를 메모리에 올리고 다시 실행한다.

</details>

-----------------------

<br>



<br>

-----------------------

### 메모리 관리 전략

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 제한된 메모리를 효율적으로 사용하고 관리하기 위한 전략
- 문제
  - 내부 단편화 : 메모리공간과 프로세스간의 크기 차이로 공간이 남게 되어 남는 공간을 활용할수 없는것.
  - 외부 단편화 : 프로세스의 크기보다 크게 메모리 공간이 남아있어도 실질적으로 메모리에 프로세스를 할당할 수 없는 것
- 연속 메모리 할당 : 프로세스를 메모리에 연속적으로 할당하는 방법 (외부 단편화 발생)
  - First Fit : 가장 먼저 만나는 빈 메모리 공간에 할당
  - Best Fit : 프로세스 크기와 빈 메모리 공간의 차이가 가장 적은 곳에 할당
  - Worst Fit : 프로세스 크기와 빈 메모리 공간의 차이가 가장 많은 곳에 할당
    - 공간이 많이 남으니 다른 프로세스가 사용할수 있을거란 생각
  - 효율성 면에서는 worst fit이 가장 좋지 못하고 first fit과 best fit은 비슷하다.
    - 하지만 best fit은 100개의 공간이 있다면 100개를 모두 확인해봐야하고, first fit은 처음 나오는 공간에 들어가면되기 때문에 first fit이 더 효율적일 수 있다.
- 페이징
  - <img width="50%" alt="image" src="https://user-images.githubusercontent.com/57162257/182620581-854c2f6b-5d2f-4096-86c8-6ed4e10c8830.png">
  - 프로세스를 일정 크기인 페이지로 나누고 물리 메모리도 동일한 크기인 프레임으로 나눈다.
  - CPU는 연속적인 논리 메모리로 메모리에 접근하지만 MMU의 페이지 테이블을 사용해서 논리메모리를 물리메모리로 변경하여 나뉘어져있는 프로세스 정보에 접근할 수 있다.
    - CPU는 메모리주소를 요청할때, 페이지 number인 논리주소(p)와 offset을 요청한다. Page table에서 논리주소를 재배치 레지스터를 통해 논리주소를 물리주소로 변경하여 요청한다.
  - 프로세스를 페이지 크기로 나누었을때, 프로세스를 나눈 페이지 중 마지막 페이지에 내부 단편화가 발생한다.
    - 페이지 크기가 1024바이트, 프로세스가 10500바이트일때, 프로세스를 11개의 페이지(11264바이트)로 나누게 되면 마지막 페이지에서 764(1024-260)바이트의 내부 단편화가 발생한다.
    - 페이지를 작게 구성할 수록 내부단편화는 줄어들지만, 페이지를 작게 구성하면 페이지 수가 많아지게 되고 페이지 테이블의 크기가 늘어나 유지 비용이 많이 들게 된다.
  - 장점
    - 메모리에 데이터가 연속적으로 저장될 필요가 없기 때문에 외부 단편화가 발생하지 않는다.
  - 단점
    - 내부 단편화가 발생할 수 있다.
  - 페이지가 클수록 좋은가?
    - 페이지가 클 수록 TLB의 히트율이 높아져 빠른 논리주소에서 물리주소 변환이 가능하다.
    - 하지만 페이지 크기가 커지기 때문에 내부 단편화가 증가하여 메모리 효율이 감소한다.
  - 페이지 주소 변경 과정
    - <img width="500" alt="image" src="https://user-images.githubusercontent.com/57162257/208838041-1aef113c-0bee-437d-a68f-6b5ba2574e09.png">
    - MMU가 빠른 메모리 주소 변환을 위해 TLB를 사용. (메모리 주소 변경을 하는 주체는 MMU)
    - MMU는 CPU 코어안에 탑재, Page Table은 (물리 메모리)RAM에 존재
      1. CPU 논리주소 요청
      2. TLB 히트 여부 확인
         1. TLB miss시, 물리 메모리에있는 PageTable에 접근하여 논리주소와 매핑된 물리주소(프레임 주소)를 가져온다
            1. PageTable에 매핑된 물리주소가 없다면 MMU가 page table갱신하여 매핑 저장(page fault)
         2. 매핑된 논리주소와 물리주소를 TLB 에 저장한다.
      3. 물리 주소 접근
    - TLB(Translation Lookaside Buffer)
      - 변환 우선참조 버퍼
      - 논리주소를 물리 주소로 변환하기 위해 매핑된 주소를 TLB에서 가져와 바로 물리주소에 접근할수 있도록 도와주는 캐시 기능
      - TLB가 없는 경우 물리 저장공간에있는 PageTable에서 매핑된 물리주소를 참조하고 다시 물리 저장공간에서 물리주소의 데이터를 가져오기 위해 총 두번의 참조가 발생하여 접근시간의 비효율 발생한다. 이를 해결해주기 위한 것
- 세그멘테이션
  - <img width="60%" alt="image" src="https://user-images.githubusercontent.com/57162257/182623165-5c221ef4-bed9-4079-b7bb-3b65a1282c67.png">
  - 페이징과 반대로 프로세스를 고정되지 않는 크기의 세그먼트 단위로 나누어 메모리에 불연속적으로 저장하는 방식.
  - CPU는 논리주소로 메모리에 접근하고 MMU의 세그먼트 테이블을 통해 논리주소를 물리주소로 변경하여 물리 메모리에 저장되어있는 프로세스 정보에 접근할 수 있다.
  - 세그먼트는 논리적 단위이기 때문에 중요도, 용도에 따라 나눌 수 있다. (보호)
    - 페이징은 동일한 크기로 나뉘어져있기 때문에 code,data,stack 구분없이 페이징될 수 있다.
    - 세그먼트는 부위별로 자를수 있기 때문에 섞이지 않는다. (code, data, stack 끼리)
    - 세그먼트 테이블의 세그먼트마다 r,w,x 비트(권한)를 부여할 수 있다.
      - r 읽기
      - w 쓰기
      - x 실행
  - 메모리 낭비 방지 (공유)
    - 같은 프로그램을 사용하는 복수개의 프로세스가 있는 경우, 세그먼트 테이블이 code영역을 같은 곳을 가리키게 만들수 있다.
    - 페이징은 영역별로 명확하게 나눌 수 없기 때문에 그럴 수 없다.
  - 장점 : 보호, 공유
  - 단점 : 외부 단편화가 발생할 수 있다.
    - 세그먼트는 가변의 크기이기 때문에 메모리 낭비가 발생할 수 있다.
- 페이징 또는 세그멘테이션을 사용하는 이유
  - 디스크에서 메모리로 데이터를 적재하는 과정에 단편화가 발생한다. 연속 메모리 할당을 통해 단편화를 해결할 수 있지만, 메모리 계산 비용이 적은 페이징 또는 세그멘테이션을 주로 사용한다.
  - 세그먼트는 보호, 공유 면에서 효과적이고 페이징은 외부 단편화 문제를 해결해준다. 그렇기 때문에 세그먼트를 페이징하는 방법을 사용한다.
    - 두번의 MMU를 거치기 때문에 속도면에서 떨어진다는 단점이 있다.


</details>

-----------------------

<br>



<br>

-----------------------

### 페이지 교체 알고리즘

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 가상 메모리를 관리하는 페이징 기법에서 페이지를 교체할때 사용하는 알고리즘
- 종류
  - FIFO (First In First Out) : 가장 먼저 들어온 페이지가 가장 먼저 교체된다.
  - LRU (Least Recently Use) : 최근에 가장 오랫동안 사용하지 않은 페이지가 교체된다. (가장 많이 사용)
    - LRU를 사용하게 되면 각 페이지의 사용빈도를 체크해야 하기 때문에 큰 오버헤드 발생 
  - LFU (Least Frequently Use) : 최근에 사용빈도가 가장 적은 페이지가 교체된다.
    - 사용횟수를 증가시키 때문에 오버헤드 발생
  - OPT (Optimal) : 앞으로 가장 오랫동안 사용하지 않을 페이지를 예상해서 교체된다. (연구목적)
  - NRU (Not Recently Use) : LRU와 동일하게 최근에 사용한지 오래된 페이지를 교체하는데, 참조비트와 변형 비트를 사용해서 LRU의 오버헤드를 줄인 알고리즘
    - 참조 비트 : 페이지에 참조했다면 1, 참조하지 않았다면 0
    - 변형 비트 : 페이지 내부 내용이 변경되었다면 1, 변경되지 않았다면 0
    - 참조 비트의 0이 변형비트의 0보다 교체 우선순위가 높다.

</details>

-----------------------

<br>



<br>

-----------------------

### Trashing

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 페이지 부재가 빈번하게 발생하면, 프로세스 실행 시간보다 페이지 교체 시간이 더 많은 상태
- 해결방법
  - 다중 프로그래밍 빈도를 줄인다.
  - Working Set 알고리즘
    - 특정 기간동안 사용되는 페이지 개수를 파악해서 해당 페이지 개수만큼의 프레임이 확보된다면 페이지를 메모리에 올리는 알고리즘
  - Page Fault Frequency 알고리즘
    - page fault 퍼센트의 상한과 하한을 지정한 후, 상한을 넘게된다면 주어지는 프레임의 개수를 늘려주고 하한을 넘게된다면 주어지는 프레임의 개수를 줄이는 알고리즘

</details>

-----------------------

<br>



<br>

-----------------------

### 메모리가 부족시 어떤 일이 일어나는가

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />




-----------------------

- 메모리가 부족한경우 가상메모리 방식을 이용한 swap이 발생
- swap이란, 저장장치 일부를 일시적으로 메모리영역으로 사용하는 것으로 메모리의 부족한 부분을 저장장치를 이용하여 메꾸어준다.
- 저장장치는 메모리영역보다 접근 속도가 느리기 때문에 성능이 떨어지게 된다.
- swap out(메모리 -> 저장장치)과 swap in(저장장치 -> 메모리)가 반복해서 작동하다보면 처리 속도가 점점 느려져 정상적인 프로세스 처리를 하지 못하게 되는 스레싱 발생

</details>

-----------------------

<br>



<br>

-----------------------

### 프로세스 상태

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- <img width="500" alt="image" src="https://user-images.githubusercontent.com/57162257/193495820-06a82f20-7d8f-41af-aa88-955971e9705c.png">
- 생성 : 프로세스가 생성되는 상태
- 준비 : 프로세스가 CPU에 실행되고 있는 상태는 아니지만 언제든지 CPU에 할당될수 있는 상태
- 실행 : CPU에 할당되어 작업을 수행하는 상태
- 대기 : 입출력과 같은 인터럽트에 의해서 프로세스가 대기중인 상태
- 종료 : 프로세스의 작업이 끝나 종료된 상태

</details>

-----------------------

<br>



<br>

-----------------------

### 스케줄러

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 프로세스를 스케줄링 하기 위해 3개의 큐가 존재한다
  - Job Queue : 존재하는 모든 프로세스를 담은 큐
  - Ready Queue : CPU할당을 대기하는 프로세스를 담은 큐
  - Device Queue : Device IO 출력을 위해 대기하고 있는 프로세스를 담은 큐

- 이 큐를 관리해주기 위해 3개의 스케줄러가 존재한다.
- 장기 스케줄러 : 디스크 내의 프로세스를 메모리에 가져올지 결정하여 프로세스를 준비 상태로 보내는 스케줄러
- 단기 스케줄러 : Ready Queue에 존재하는 프로세스 중 CPU에 할당될 프로세스를 정하는 스케줄러
- 중기 스케줄러 : CPU에 할당되기 위해 너무 많은 프로세스가 몰리게 되면 메모리를 관리하기 위해 프로세스를 디바이스 영역으로 보내는 스케줄러
  - Swap out : 프로세스를 메모리에서 물리 저장공간 영역으로 보냄
  - swap in : 물리 저장공간에 있는 프로세스를 메모리로 다시 들임

</details>

-----------------------

<br>



<br>

-----------------------

### 현대 OS

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />




-----------------------

- 현대 OS에서는 가상 메모리를 통해 장기,중기 스케줄러를 사용하지 않고 단기 스케줄러만 사용한다.
- 실행가능한 프로세스를 가상 메모리에 올리고, 페이지 교체를 통해 실행에 필요한 부분만 메모리에 올려 CPU에 실행한다.
- 메모리가 부족한 경우 물리 저장공간에 메모리에 할당된 프로세스를 빼내어 주소 공간을 확보하는것
  - 물리 저장공간에 프로세스를 저장하여 접근하기 때문에 메모리에서 접근하는 것보다 속도가 느리다.


</details>

-----------------------

<br>



<br>

-----------------------

### 인터럽트

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- CPU가 프로세스를 처리하고 있을 때, 입출력이나 예외가 발생하면 해당 이슈를 처리하기 위해 CPU에게 알려주는 것
- 종류
  - 하드웨어 인터럽트
    - CPU외부의 디스크나 주변장치로 부터 전기적신호로 요청되어지는 인터럽트
    - 기계 검사 인터럽트
      - 갑작스런 정전이나, 내부 기계적 문제가 발생한 경우
    - 외부 인터럽트
      - 타이머로 인해 의도적으로 프로그램이 종료된 경우
    - 입출력 인터럽트
      - 입출력이 요청된 경우
    - 프로그램 검사 인터럽트
      - 보호된 주소에 접근하거나 불법 명령과 같은 프로그램 문제 발생한 경우
  - 소프트웨어 인터럽트
    - CPU내부에서 잘못된 데이터나 잘못된 명령어를 수행한 경우
    - 존재하지 않는 메모리 주소 접근
    - 나눗셈시 0으로 나누는 경우
- 우선순위
  - 주로 외부 인터럽트를 우선적으로 처리한다
  - 전원 이상 > 타임아웃 > 입출력 > 예외 > 감시 프로그램
- 우선순위 판별
  - 소프트웨어 방법
    - 내부적으로 인터럽트 요청을 확인하여 우선순위가 높은 인터럽트를 찾는다.
    - 우선순위 변동이 쉽고 간단하다. 하지만 하드웨어 방법에 비해 처리가 느리다.

  - 하드웨어
    - 인터럽트 요청 장치와 CPU사이의 장치식별 번호를 통해 높은 우선순위의 인터럽트를 찾는다.
    - 복잡하며 소프트웨어보다 비효율적이지만, 처리속도가 빠르다.
- 인터럽트 수행 과정
  1. CPU 실행 중 인터럽트 발생
  2. 수행중인 프로세스를 멈추고 PCB에 상태를 저장하고 pc에 다음 수행할 작업을 저장한다.
  3. 인터럽트 백터를 읽어 해당 인터럽트의 인터럽트 서비스 루틴(인터럽트 핸들러)으로 이동
     1. 인터럽트 백터
        1. 인터럽트 발생시 처리해야할 인터럽트 서비스 루틴의 주소를 인터럽트 별로 보관하고 있는 테이블
  4. 인터럽트 서비스 루틴 (인터럽트 핸들러)를 통해 루틴을 실행하여 문제를 해결한다.
     - 인터럽트 서비스 루틴에는 특정 인터럽트 발생시 처리해야할 내용이 이미 프로그램되어 있기 때문에 인터럽트 백터를 이용해 인터럽트 서비스 루틴의 주소를 알아내고 해당 인터럽트에 대해 수행해야할 프로그램을 수행하여 문제를 해결한다.
     - 입출력인 경우 입출력 관리자에게 입력데이터는 메모리로, 출력데이터는 메모리에서 출력을 요청함으로써, CPU는 다른 작업을 수행하고 입출력 작업 완료시 입출력 관리자는 CPU에게 처리 완료 신호를 전달한다.
  5. 완료 후, 인터럽트를 해제하고 저장된 프로세스 정보를 가져와 다시 수행.
- Polling
  - CPU가 작업하다 입출력 명령을 받게 되면 직접 입출력장치에서 작업을 수행하는 방법
  - CPU가 외부 작업을 주기적으로 확인하고 입출력을 하는 동안 다른 작업을 하지 못하므로 비효율이 발생한다.


</details>

-----------------------

<br>



<br>

-----------------------

### CPU 스케줄링

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 대기하고 있는 프로세스를 CPU에 할당시키는 스케줄링으로써, 단기 스케줄러에 의해 수행되며 CPU의 선점 여부에 따라서 선점 스케줄링과 비선점 스케줄링으로 나뉜다.
- 종류
  - 선점 스케줄링 : CPU의 제어권을 뺏음
  - 비선점 스케줄링 : CPU의 제어권을 뺏지 않음
- 비선점 스케줄링
  - FCFS (First Come First Service) : CPU를 요청한 순서대로 CPU에 할당
  - SJF (Short Job First) : CPU 요구량이 가장 적은 프로세스부터 할당
  - HRN (Highest Response-Ratio Next) : 우선순위를 계산하여 점유 불평등을 보완하는 방법
    - 우선순위 = (대기시간 + 실행시간)/실행시간
- 선점 스케줄링
  - 라운드 로빈 (Round Robine) : FCFS처럼 CPU를 요청한 순서대로 할당하지만, 각 프로세스마다 할당가능한 시간을 주고 선점하는것.
  - SRT (Shortest Remaning Time) : SJF처럼 CPU요구량이 적은 프로세스부터 할당하지만, 중간에 더 적은 프로세스가 있다면 선점하는것.
  - 다단계 큐 (Multi-level Queue) 
    - 각 큐마다 절대적인 우선순위와 스케줄링(RR)을 적용하며 프로세스는 종류와 우선순위에 따라 큐에 배치된다.
    - 각 큐마다 프로세스는 이동할 수 없기 때문에 스케줄링 부담은 적지만 유연함이 부족하다.
    - 내부적으로 RR을 사용하며 우선순위가 가장 낮은 큐는 FCFS를 사용한다.
  - 다단계 피드백 큐 (Multi-level Feedback Queue)
    - 각 큐마다 절대적인 우선순위와 스케줄링(RR)을 적용한다.
    - 하지만 각 큐마다 프로세스가 이동할 수 있기 때문에 기아현상을 방지하고 유연하게 CPU에 할당할수 있다.
    - 작업을 가장 높은 우선순위큐에 저장한 후, RR수행후 수행시간이 남는다면, 다음 큐로 이동시켜 더 많은 RR의 시간을 보장받게 한다.
    - 공평하게 작업 시간을 할당하고 이후 더 많은 시간을 할당해 줌으로써, CPU의 응답시간과 반환시간을 향상시킬수 있다.
    - 내부적으로 RR을 사용하며 주어진 시간을 사용했는데 아직 bursttime이 남아있다면 우선순위가 낮은 큐로 이동하여 더 많은 시간을 할당받는다. 우선순위가 가장낮은 큐는 FCFS를 수행한다.

</details>

-----------------------

<br>



<br>

-----------------------

### 커널

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 운영체제의 핵심이 되는 부분으로써, 컴퓨터자원을 관리하는 역할. 
- 보안을 위해 운영체제를 커널 영역과 시스템 영역으로 분리하였고 컴퓨터 자원을 사용하기 위해서는 커널 영역에서만 사용할수 있도록 기능을 제한.

</details>

-----------------------

<br>



<br>

-----------------------

### System Call

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 운영체제는 기능을 제한하여 보안을 하기 위해 커널 모드와 사용자 모드로 나누어 구동한다.
- 시스템 콜은 커널 모드의 기능을 사용자 모드에서 사용하기 위해 프로세스가 운영체제에게 요청하는 인터페이스.
  - 사용자 모드는 직접 하드웨어에 접근하지 못하기 때문에 system call을 요청한다.

- 사용자 프로세스는 사용자 모드에서 작업을 수행하다 시스템 자원을 사용해야할때, 시스템 콜을 호출하여 커널모드로 전환되어 작업하고 다시 사용자 모드로 변경
- 분류
  - 프로세스 제어
    - fork
      - 프로세스의 생성과 제어를 위한 시스템 콜
      - 새로운 프로세스(자식)를 만드는 것
      - PCB/프로세스 메모리 구조가 복사된다.
        - 새로운 프로세스는 원래 프로세스와 똑같은 코드를 가지고 있다.
    - exec
      - 프로세스의 생성과 제어를 위한 시스템 콜
      - 프로세스를 새로운것으로 덮어씌워 실행시키는 것
      - 새로운 메모리를 할당하지 않고 현재 프로세스를 덮어 씌워 실행
        - 새로운 프로세스를 생성하는 것이 아닌 기존 PID를 사용하는 새로운 프로세스로 사용되는것.
  - 파일 조작
    - 파일 생성, 삭제, 읽기, 쓰기
  - 장치 조작
    - 장치 요청, 해제, 해드웨어 상태 정보
  - 정보 관리
    - 시간 관리, 프로세스 정보 불러오기
  - 통신
    - 메시지 송수신 ,커뮤니케이션 설정
  - 보호
    - chmod와 같은 파일 권한 설정
- cp in.txt out.txt
  1. 키보드로 명령어 작성시 `I/O 시스템콜` 호출
  2. 읽으려는 In.txt파일이 현재 디렉터리에서 `접근가능한지 검사하는 시스템 콜` 호출
  3. 접근이 불가능하면 `에러 발생시키는 시스템콜` 호출
  4. 복사하려는 out.txt파일이 `중복되는지 검사하는 시스템 콜` 호출
  5. 중복이라면 덮어씌울건지 물어봄.

</details>

-----------------------

<br>



<br>

-----------------------

### PCB (Process Control Block)

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- PCB는 프로세스의 정보인 Process Metadata를 저장하는 곳
- OS스케줄러에 의해 context switching되는 정보 단위
- 커널내에 존재
- Process Meta Data
  - Process ID
  - Process State
  - Process Priority
  - Program Count : 다음에 실행할 명령어 주소

</details>

-----------------------

<br>



<br>

-----------------------

### TCB(Thread Control Block)

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />




-----------------------

- 스레드 라이브러리에 의해 context swtiching되는 정보단위
- Thread Meta Data
  - Thread ID
  - Thread Status
  - PC
  - Process ID


</details>

-----------------------

<br>



### PCB 필요한 이유

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- context switching시 실행중인 프로세스의 정보를 PCB에 저장하고 다음 프로세스를 실행시킬 때 PCB를 이용해 프로세스의 정보를 CUP의 Register에 불러오고 실행

</details>

-----------------------

<br>



### PCB 저장과정

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

1. 프로그램 실행
2. 프로세스 생성
3. 프로세스 메모리 (code, data, stack) 공간 할당
4. 프로세스의 메타데이터가 커널의 PCB에 저장

</details>

-----------------------

<br>



<br>

-----------------------

### Context Switching

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 스케줄링에 따라 실행중인 프로세스의 정보를 저장하고 새로운 프로세스의 정보를 CPU에 넘겨주는 작업 과정
- CPU가 교체되는 프로세스의 정보를 PCB에 저장하고, 새로운 프로세의 정보를 PCB에서 불러와 CPU의 레지스터에 적재
- 인터럽트가 발생하면 인터럽트 처리를 수행하기 전에 발생
- 프로세스
  - 프로세스의 context switching은 실행중이던 프로세스의 정보를 PCB에 저장하고 캐시를 지운 다음에 CPU에 할당시킬 프로세스의 정보를 PCB에서 가져와 CPU에 적재시켜야한다.

- 스레드
  - 스레드의 context switching은 실행중이던 스레드의 정보를 TCB에 저장해한다. 하지만 프로세스 context switching과는 다르게 메모리를 공유하고 있어 초기화 비용을 줄여 빠르게 context switching이 발생한다.


</details>

-----------------------

<br>



<br>

-----------------------

### Context Switching 발생 과 비 발생

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 발생
  - Dispacher에 의해서 Ready Queue의 프로세스가 CPU에 할당될 때
  - Time 인터럽트 발생 시
  - 입출력 요청 system call 발생 시
- 비발생
  - Time 인터럽트를 제외한 인터럽트
  - 입출력 요청 system call을 제외한 system call
  - 두 경우시 모두 운영체제가 커널모드로 변경되어 실행될 뿐 실행중인 프로세스를 중단했다가 요청을 처리한 후 다시 실행한다.

</details>

-----------------------

<br>



<br>

-----------------------

### Context Switching 오버헤드

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- context swtiching시 새로운 프로세스 정보를 CPU의 레지스터에 적재할 때 CPU는 아무런 작업을 할 수 없다.
- 이 때를 context switching의 오버헤드라고 한다.
- 해결 방안
  - 다중 프로그래밍 수준을 낮추어 context switching 발생 빈도를 줄인다.
  - 스레드를 이용해서 context switching 부하를 최소화한다.
    - 스레드는 프로세스와 다르게 공유 메모리 공간이 있기 때문에 context switching이 발생해도 캐시 데이터를 공유 자원에 저장하고 있고 메모리 주소를 변경하지 않아도 되기 때문에 비용적으로나 시간적으로나 효율적이게됩니다.

</details>

-----------------------

<br>



<br>

-----------------------

### Dispatcher

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 스케줄러가 Ready 상태의 프로세스 중 하나를 CPU에 할당시켜 Run 상태로 변경하는 요소를 Dispatcher라고 한다.
- dispatcher가 Ready 상태의 프로세를 CPU에 할당하는데 까지 걸리는 시간을 Dispatcher Latency라고 한다.
- Ready상태의 프로세를 Run상태로 변경하는 것을 Dispatcher가 수행하고 실행중인 프로세스의 정보를 PCB에 저장하고 새로운 프로세스의 정보를 PCB에서 가져와 실행시키는 모든과정을 Context Swtiching이라고 한다.
- 즉, Context Switching과정에 Dispatch 과정이 포함된 구조이다.

</details>

-----------------------

<br>



<br>

-----------------------

### 교착상태(Dead Lock)

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 두개 이상의 프로세스나 스레드가 자원을 점유한 상태에서 서로의 자원을 점유하기 위해 기다려서 무한한 대기가 발생하는 것.
- 발생 조건
  - 상호 배제 (Mutual Exclusion) : 하나의 자원에 여러 프로세스의 동시접근을 막는것.
  - 점유와 대기 (Hold and Wait) : 하나의 자원을 점유하고 다른 프로세스의 자원을 요청하는 것.
  - 비선점 (Non Primitive) : 점유하고 있는 자원의 제어권을 뺏을수 없는 것
  - 환영 대기 (Circle Wait) : 각 프로세스가 다음 프로세스가 요청하고 있는 자원을 점유하는 것
- 해결방법
  - 예방 (Prevention) : 4가지의 발생 조건 중 하나를 해결하여 예방하는 것.
  - 회피 (Avoidance) : 발생하지 않도록 알고리즘을 설계하여 교착상태를 피하는 것.
    - 은행원 알고리즘 : 자원을 할당한 후에도 안정 상태로 유지되는지 사전에 검사하여 교착상태를 회피하는 것.
  - 탐색 (Detection) : 자원할당 그래프를 모니터링 해서 교착상태를 탐지하는 것
  - 회복 (Recovery) : 교착 상태가 탐지되면 복구를 실행.

</details>

-----------------------

<br>



<br>

-----------------------

### IPC (Inter Process Communication)

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 독립적인 프로세스간 통신하기 위한 설비
- 프로세스는 커널에서 제공해주는 IPC 설비를 통해 프로세스간 통신이 가능하다.
- IPC에서는 프로세스간 데이터를 동기화하고 보호하기 위해서 뮤텍스와 세마포어를 사용한다.
- 종류
  - 익명 Pipe
    - 두개의 프로세스를 연결하여 한쪽에서는 쓰기만, 다른 한쪽에서는 읽기만 가능한 반이중 통신.
    - 전이중 통신을 위해서는 2개의 파이프가 필요하다.
    - 통신할 프로세스간의 관계가 명확한 상태에서만 사용할 수 있다. (부모-자식 프로세스 관계)
  - Named Pipe
    - 익명 Pipe와 마찬가지로 반이중 통신.
    - 전이중 통신을 위해서는 2개의 파이프가 필요하다.
    - 두 프로세스간 관계가 명확하지 않아도 통신이 가능하다.
  - Message Queue
    - Pipe는 데이터의 흐름이고 Message Queue는 메모리 공간을 가지는 자료구조
    - 메세지에 데이터를 담은 후 번호를 남기고 전역으로 관리되는 하나의 큐를 통해 여러 프로세스가 동시에 데이터를 주고 받을수 있다.
    - 큐는 커널영역에 존재하기 때문에 프로세스가 종료되어도 데이터가 삭제되지 않는다.
    - 양방향 통신 가능
  - Shared Memory
    - Pipe와 Message Queue는 통신을 위한 설비라면, Shared Memory는 데이터 자체를 공유하기 위한 설비이다.
    - 프로세스간 메모리 영역(RAM)을 공유해서 같이 사용할 수 있도록 허용한다.
      - OS가 프로세스의 독립적인 메모리 공간에 공유 메모리 주소를 매핑한 매핑 테이블 정보를 저장하여 공유할 수 있다.
    - 장점
      - IPC기법중 가장 빠른 처리 속도
      - 메모리를 공유함으로써 메모리 효율적 사용
    - 단점
      - 항상 동일한 RAM에서만 공유 가능
      - 가시성 문제(로컬 캐시)
  - Socket
    - 네트워크 소켓 통신을 통해 데이터 공유
    - 네트워크상에서 호스트칸 데이터를 전달하기 위해 소켓을 사용한다.
      - 소켓 : 네트워크상에서 데이터를 내보내거나 받기위한 실제적 창구역할
        - 프로토콜(TCP,UDP) + IP + 포트 로 정의할 수 있다.
      - 포트 : 네트워크상에서 통신을 위해 호스트 내부 프로세스가 할당받는 식별자

</details>

-----------------------

<br>



<br>

-----------------------

### Race Condition & Critical Section

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- Race Condition
  - 하나의 자원에 대해 여러 프로세스/스레드가 접근하였을때, 예상한 결과를 보장할수 없는 경우
- Synchronization
  - 여러 프로세스/스레드가 공통 자원에 접근하여도 일관성있는 데이터를 유지하는 것

- Critical Section (임계 영역)
  - 일관성있는 데이터를 제공하기 위해 공통자원에 하나의 프로세스/스레드의 접근을 허용하여 실행하도록 하는 영역
- 임계 영역을 문제를 해결하기 위한 조건
  - 상호 배제 (Mutal Exclusion) : 하나의 프로세스가 임계영역에 있다면 다른 프로세스의 접근을 막는것.
  - 진행 (Progress) : 임계영역을 사용하는 프로세스가 없을때 프로세스의 접근이 가능하다
  - 한정 대기 (Bounded Waiting) : 임계영역에 들어갔다온 프로세스는 다음 접근에 제한을 받음.

</details>

-----------------------

<br>



<br>

-----------------------

### Synchronization (동기화)

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />




-----------------------

- 프로세스 동기화 : 하나의 자원을 동시에 여러 프로세스가 동시에 접근하는것을 제어
- 스레드 동기화 : 하나의 코드블럭 또는 메소드를 여러 스레드가 동시에 접근하는 것을 제어
- 동기화 방법 : 뮤텍스, 세마포어

</details>

-----------------------

<br>



<br>

-----------------------

### 스핀락&뮤텍스 & 세마포어

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />




-----------------------

- 스핀락

  - 공유 자원에 대한 프로세스, 스레드의 동기화를 위해서는 락이 필요합니다.

  - 락을 얻을때까지 반복적으로 락을 요청하는 것을 스핀락이라고 합니다.

  - ```java
    int lock = 0;
    class critical{
      while(test_and_set(lock) == 1);
      //...critical_section
      lock = 0;
    }
    
    int test_and_set(int lock){
    	int old_lock = lock;
      lock = 1;
      return old_lock;
    }
    ```

    - 스레드가 critical_section에서 작업을 위해 test_and_set을 호출하여 lock을 얻습니다.
    - lock이 0인 경우 반복문을 빠져나와 critical_section을 수행하고 lock을 반납합니다.
    - test_and_set은 CPU에 의해 관리되는 atomic한 메소드이기 때문에 T1, T2가 동시에 test_and_set을 호출하여도 CPU레벨에서 lock을 관리하여주기 때문에 동시성 문제를 방지할 수 있습니다.

  - 단점

    - 스핀락의 경우 락을 얻을때까지 반복적으로 락을 요청하기 때문에 CPU낭비가 심합니다.

- 뮤텍스

  - 오직 한개의 프로세스, 스레드가 공유자원에 접근하는 것을 허용하여 동기화 시켜주는 방법입니다.

  - 뮤텍스는 락을 얻기위해 반복적으로 락을 요청하는 것이 아닌, 락이 풀릴때까지 대기하였다가 락이 풀리게 되면 락을 얻어 critical_section을 수행합니다.

  - ```java
    class Mutex{
      //스레드의 접근을 허용하게 하는 값, value가 0이면 다른 스레드가 공유자원에 접근하지 못합니다.
      int value=1;
      //value도 여러 스레드가 경합하는 자원이기 때문에 value에 critical_section을 제공하기 위한 값.
      int guard=0;
    }
    
    class critical{
      mutex_lock();
      //...critical_section
      mutex_unlock();
    }
    
    void mutex_lock(){
      //내부적으로 guard를 1로 변경하여 다른 스레드의 value접근을 제한합니다.
      while(test_and_set(guard));
      
      //다른 스레드가 공유자원을 사용하고있는 경우
      if(value==0){
        //...현재 스레드를 큐에 넣어 대기시킵니다.
      }else{
        value = 0;
      }
      guard = 0;
    }
    
    void mutex_unlock(){
      while(test_and_set(guard));
      
      if(/*큐에 대기중인 스레드가 있는 경우*/){
        //...대기중이던 스레드 하나를 깨워 작업을 실행시킵니다.
      }else{
        value = 1;
      }
      guard = 1;
    }
    ```

- 세마포어(Semaphore)

  - 하나 이상의 프로세스,스레드가 공유자원에 접근할수 있도록 제한하는 동기화 방법

  - 특징

    - 작업에 대해 우선순위를 적용할수 있는 `시그널 매커니즘`
    - 여러 프로세스에 적용한다.

  - ```java
    class Semaphore{
      int value = 2;
      int guard = 0;
    }
    
    class critical{
      semaphore_wait();
      //...critical_section
      semaphore_signal();
    }
    
    //공유자원 락 걸기
    void semaphore_wait(){
      while(test_and_wait(guard));
      
      //공유자원에 접근가능한 수가 0이라면 접근이 가능할때까지 큐에 대기합니다.
      if(value == 0){
        //...현재 스레드를 큐에 저장하여 대기시킵니다.
      }else{
        //공유자원에 접근가능하다면 접근가능 수인 value를 하나 감소시킵니다.
        value -= 1;
      }
      guard = 0;
    }
    
    //공유자원 락 해제
    void semaphore_signal(){
      while(test_and_wait(guard));
      
      if(/*큐에 대기중인 스레드가 있다면*/){
        //...하나의 스레드를 꺼내어 작업을 수행합니다.
      }else{
        //공유자원을 사용하였다면 공유자원 접근 가능 수를 하나 증가시킵니다.
        value += 1;
      }
      guard = 0;
    }
    ```

  - 시그널 매커니즘

    - 수행할 작업에 대해 우선순위를 지정해주는 방법
    - <img width="1702" alt="image" src="https://user-images.githubusercontent.com/57162257/205202165-3a2c2fb0-f2e9-48f1-999a-e6bbb4be24bb.png">
      - 출처 : https://www.youtube.com/watch?v=gTkvX2Awj6g

    - Wait()은 락을 걸어주는 메서드(value--), signal()은 락을 풀어주는 메서드(value++)
    - task3는 task1이 수행되어야만 작업을 수행할수 있다(task1 우선순위 > task3)
      - task1의 작업이 끝나고 signal()이 수행되어야 value가 0이 아니게 되기때문에 wait()이 수행되어 task3가 작업할수 있습니다.

    - 세마포어는 같은 프로세스가 아닌 다른 프로세스에서도 적용이 가능합니다.


</details>

-----------------------

<br>



<br>

-----------------------

### Mutex vs 스핀락 / Mutex & Semaphore

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />




-----------------------

- Mutex vs 스핀락
  - 스핀락은 락을 얻기위해 반복적으로 락을 요청하기 때문에 CPU낭비가 심하고 Mutex는 락을 얻을때까지 대기하기 때문에 Mutex가 성능이 좋습니다.
  - 하지만 멀티코어이거나 critical_section의 작업이 context switching보다 빠르게 끝나는 경우

    - 멀티코어
      - 특정 코어에서 락을 대기하고 다른 코어에서 락을 사용하고있고 락을 해제했을때 바로 락을 얻을 수 있기 때문에 효율적입니다.

    - context swtiching보다 critical_section작업이 빠른경우
      - Mutex로 인해 스레드가 대기상태가 될때와 대기상태에서 꺼내어 질떄 context switching이 두번 발생하게 되고 스핀락으로 대기하는것은 한번의 context switching이 발생하기 때문에 더 효율적입니다.

- Mutex & Semaphore
  - 상호배제만 사용하길 원한다면 Mutex
  - 작업간의 실행 순서 동기화가 필요하다면 Semaphore


</details>

-----------------------

<br>



<br>

-----------------------

### Mutex vs Binary Semaphore

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />




-----------------------

- Mutex와 Binary Semaphore는 코드상으로는 유사해보입니다. (하나의 프로세스, 스레드만 공유자원에 접근이 가능하기 때문)
- Mutex는 락을 가지고있는 개체가 락을 해제할수 있지만, Semaphore는 락을 가지지 않은 개체도 락 해제가 가능합니다.
  - 세마포어의 시그널 매커니즘에 서로 다른 프로세스에서 value를 다룰수 있습니다.

- Mutex는 priority inheritance를 가지지만 Semaphore는 가지지 않습니다.
  - Priority Inheritance
    - <img width="779" alt="image" src="https://user-images.githubusercontent.com/57162257/205206318-36852830-ad39-472e-8c05-fce01273a740.png">
    - 우선순위가 낮은 프로세스(P1)가 락을 가지고 있을때, 우선순위가 높은 프로세스(P1)가 락을 요청한다면 락이 걸려있어 P1은 공유자원을 사용하지 못합니다.
    - 이때 P1은 P2에 의존하게되고 우선순위가 낮은 P2는 P1만큼의 우선순위를 가지게 되고 빠르게 critical_section작업을 수행하고 P1에게 락을 넘겨주게됩니다.


</details>

-----------------------

<br>



<br>

-----------------------

### CPU 성능 척도

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- CPU Utilization (이용률) : CPU가 쉬지않고 일한 시간
- Thorughput (처리량) : 단위 시간 처리량
- Turnaround Time (소요시간) : 프로세스의 CPU사용시간 + 대기 시간
- Waiting Time (대기시간) : 프로세스가 Ready Queue에서 대기한 총 시간
- Response Time (응답시간) : 프로세스가 최초 Ready Queue에 들어가서 CPU에 할당되기까지 걸린 시간

</details>

-----------------------

<br>



<br>

-----------------------

### 캐시

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- CPU가 주기억장치에 접근하는 횟수를 줄여주기 위해 주기억장치의 특정 정보를 저장하는 저장소로 캐시 지역성을 통해 주기억장치의 특정 정보를 캐시에 저장한다.
- 캐시 적중률 (Hit)
  - CPU가 필요로하는 데이터를 캐시가 가지고 있다면 더 빠르게 데이터를 가져올 수 있다.
  - 캐시 적중률 = 캐시 메모리 적중 횟수 / 전체 메모리 참조 횟수
- 메모리 접근 순서
  - 캐시 -> 주기억장치 -> 보조 기억장치
- 캐시 종류
  - L1
    - 고성능이지만 고가인 작은 용량의 캐시, 명령어 캐시와 데이터 캐시로 나누어져 있다.
      - 명령어 캐시 (I Cache) : 메모리의 텍스트영역을 다루는 캐시 (공간지역성이 높다)
      - 데이터 캐시 (D Cache) : CPU가 직전에 사용한 데이터나 바로 실행해야하는 중요한 파일 저장 (시간지역성이 높다)

    - 두 캐시로 나뉘어져있기 때문에 서로 다른 지역성을 이용할 수 있고 명령어와 데이터를 동시에 읽어올수 있어 CPU의 파이프라이닝 성능을 향상시킬 수 있다.

  - L2
    - L1캐시에서 데이터를 가져오기 위한 캐시로, L1보다 용량이 크고 별도의 캐시로 나뉘어지지 않는다.

  - L3
    - 멀티코어 시스템에서 코어들이 공유하는 캐시 메모리

  - 각 캐시는 CPU내부에 존재한다.
- 캐시 내부구조
  - <img width="450" alt="image" src="https://user-images.githubusercontent.com/57162257/194767407-9d1cf78b-6743-4dd5-8188-8a5c071793b6.png">
  - S개의 집합으로 이루어져있고 각 집합은 E개의 캐시 라인을 저장하고 있다.
  - 하나의 캐시라인은 주기억장치에서 가져오는 하나의 블럭이다.
    - 유효성을 나타내는 v
    - 동일 집합에서 블럭을 나타내는 tag
    - B크기의 블럭

  - Cache Size = v + tag + B (byte)
- 캐시 정책
  - 주기억장치의 데이터 블럭은 캐시 메모리의 특정 영역에만 저장될 수 있다. (`해시`)
  - 예를 들어 주기억장치의 2번째 블럭(0001)은 캐시의 두번째 집합(001)에만 저장될 수 있다. 그 이유는, 주기억장치의 블럭이 캐시의 어느 영역에나 저장되게 된다면, CPU에서 특정 블럭을 요청할때 캐시에서 해당 블럭이 있는지 확인하기 위해 선형탐색으로 모든 캐시 메모리를 확인해야한다.O(S*E)
    하지만 특정 집합에만 저장될 수 있게 된다면 해당 집합에 대해서만 찾으면 되기 때문에 더 빠르게 찾을 수 있게 된다. O(E)
- 캐시 읽기 동작
  - <img width="200" alt="image" src="https://user-images.githubusercontent.com/57162257/194767800-8eaa5e13-78d0-48ec-82d7-368a30cb87d9.png">
  - 과정
    1. CPU가 참조하고 싶은 메모리 주소를 전달한다.
    2. 캐시는 메모리 주소의 s bits를 통해 캐시의 해당 집합으로 이동한다.
    3. 메모리 주소의 t bits와 캐시라인의 tag들을 비교한다.
    4. 캐시 라인의 v bits를 통해 유효성 검사를 확인하고 유효하고 tag를 찾게 되면 HIT
  - 캐시 히트인 경우 메모리 주소에서 요청하는 b bits를 통해 해당 블럭에서 블럭을 전달.
  - 캐시 미스인 경우 주기억장치에서 해당 블럭을 불러와 캐시에 저장하고 CPU에 전달.
    - 만약 캐시에 저장공간이 부족하게 되면 FIFO, LFU, LRU 알고리즘을 통해 블럭을 교체한다.
- 캐시 쓰기 동작
  - 수정하려는 블럭이 히트인 경우
    - write through
      - 캐시의 내용을 수정하고 주기억장치에도 동기화하여 캐시와 주기억장치의 동시성 보장
        - 하지만 주기억장치의 데이터를 update하기 까지 대기 시간이 발생하여 성능이 떨어진다.

    - write back
      - 캐시의 내용을 수정하고 해당 캐시 블럭을 교체할때 주기억장치 블럭의 내용을 동기화한다.
      - 주기억장치의 접근을 최소화하기때문에 성능은 향상되지만 주기억장치 블럭의 수정이 필요한지 여부를 알려줄 추가 bit가 필요하고 캐시와 주기억장치간의 일관성 불일치 문제가 발생할 수 있다.
  - 수정하려는 블럭이 미스인 경우 (allocate - 할당)
    - write allocate
      - 주기억장치 블럭의 내용을 수정한 뒤 캐시로 가져오는 방식
  
    - no-write allocate
      - 주기억장치 블럭만 수정하고 캐시에 올리지 않는 방식
- 캐시의 성능
  - 평균접근시간 = Hit latency + Miss Rate * Miss latency
    - 캐시의 크기를 줄여 Hit latency를 줄이거나 캐시 크기를 늘여 Miss Rate를 줄여 캐시의 성능을 향상시킬수 있다.
- Associatvie Cache (연관 캐시) = 캐시 구조
  - CPU가 메모리 주소를 요청했을때 서로 같은 캐시메모리 주소이지만 다른 주기억장치 주소라면 충돌이 발생한다.
  - Direct Mapped Cache
    - 주기억장치의 여러 주소가 캐시 메모리의 한 주소에 대응되는 1:n 방식
    - 캐시의 한 위치에 하나의 block만 들어갈 수 있다. (Miss rate 증가)
    - <img width="200" alt="image" src="https://user-images.githubusercontent.com/57162257/195005471-06ab5bb7-e253-46f9-89b0-3c6e83b634e1.png">
    
  - Fully Associative Cache
    - 비어있는 캐시 공간이 있다면 아무 메모리 주소를 저장할 수 있다.
    - 모든 캐시 메모리를 탐색해야하기 때문에 검색 속도는 느리지만 저장은 빠르다.
    
  - Set Associative Cache
    - 하나의 캐시 라인(캐시 주소)에 2개 이상의 메인 메모리 주소를 할당하여 DMC보다 검색은 느리지만 저장은 빠르고 Fully Associatee Cache보다는 저장은 느리지만 검색은 빠르다.
    - 캐시는 여러 block을 set으로 묶고 여러 set으로 구성되어있다.
    - 메인 메모리의 block은 저장될 set은 정해져있지만, set 내부에서 위치는 자유롭다.
    - <img width="200" alt="image" src="https://user-images.githubusercontent.com/57162257/195005499-6bf835a6-6da0-4a9e-aa28-addc8f18129b.png">


</details>

-----------------------

<br>



<br>

-----------------------

### 캐시 지역성

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 캐시는 사용 예상되는 데이터를 저장하여 CPU의 처리속도를 빠르게 도와준다.
- CPU가 캐시를 참조했을때 쓸모있는 정보에 따라 처리속도가 달라진다.
- 적중률(Hit rate)를 극대화시키기 위해 데이터의 지역성을 사용한다.
- 지역성
  - 시간 지역성 : 최근에 참조된 데이터는 곧 다시 참조되는 특징
  - 공간 지역성 : 참조된 데이터의 인근 데이터가 참조되는 특징
- 지역성을 이용해 데이터를 캐시에 담고있다면 적중률을 높일수 있다.

</details>

-----------------------

<br>



<br>

-----------------------

### 기아현상 & 에이징 기법

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 기아현상 : 프로세스의 할당 우선순위가 낮아 지속적으로 CPU의 할당을 받지 못하는 문제
- 에이징 기법 : 기아현상을 해결하기 위한 방법으로 프로세스의 할당 우선순위를 대기시간에 비례하여 점진적으로 높여주어 CPU에 할당될수 있도록 하는 방법

</details>

-----------------------

<br>



<br>

---

### Sync, Async, Blocking, NonBlocking

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />




-----------------------

- Sync는 두 작업이 시간의 흐름에 따라 연속적으로 수행되는 것으로 다른 작업의 완료여부를 신경쓴다.
- Async는 두 작업이 시간의 흐름에 상관하지 않고 독립적으로 작업을 수행하는 것으로 다른 작업의 완료여부를 신경쓰지 않는다.
- Blocking은 함수를 호출하였을때 호출된 함수가 제어권을 넘겨주지 않아 호출한 함수가 완료될때까지 대기하는 것.
- NonBlocking은 함수를 호출하였을때 호출된 함수가 제어권을 넘겨주어 호출한 함수도 독립적으로 다른 작업을 수행하는 것.
- Async, Sync와 Blocking, NonBlocking은 비슷한 말이지만 서로의 관심사에 따라 분류할수 있다.
  - Sync와 Async는 호출한 작업의 완료여부를 신경쓰는지 여부가 관심사이다.
  - Blocking과 NonBlocking은 호출된 함수가 호출한 함수에게 제어권을 주는지 여부가 관심사이다.

- Sycn Blocking
  - A함수가 B함수를 호출하였을때, B는 A에게 제어권을 넘겨주지 않고 A는 B가 완료될때까지 대기한다.
  - file.read(), file.write()

- Async NonBlocking
  - A함수가 B함수를 호출하였을떄, B는 A에게 제어권을 넘겨주고 A와 B는 각자의 작업을 독립적으로 수행하고 B가 작업을 완료하면 callback을 전달한다.
  - java의 CompletionHandler, Node.js

- Sync NonBlocking
  - A함수가 B함수를 호출하였을때, B는 A에게 제어권을 넘겨주고 A와 B는 독립적으로 작업을 수행하고 A는 B의 완료여부를 계속 확인한다.
  - java의 Futer.isDone()

- Async Blocking
  - A함수가 B함수를 호출하였을때, B는 A에게 제어권을 넘겨주지 않고 A는 B의 완료여부에 상관하지 않고 작업이 끝날때까지 다른 작업을 수행하지 못한다.


</details>

-----------------------

<br>