# 운영체제(OS)



<br>

-----------------------

### 운영체제란

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 컴퓨터의 하드웨어와 소프트웨어가 통신하고 작동할수 있도록하는 소프트웨어 환경
- 목적
  - 컴퓨터의 계산 활동을 관리하여 컴퓨터 시스템이 제대로 작동하도록 한다.
  - 프로그램의 개발 및 실행을 위한 환경 제공.

</details>

-----------------------

<br>



<br>

-----------------------

### 프로세스와 스레드

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 프로세스 : 메모리에 올라와서 실행되는 프로그램의 실행 단위
- 스레드 : 프로세스 안에서 역할을 수행하는 흐름 단위, 다른 스레드와 자원 공유 가능
- 스레드는 메모리에서 stack만 할당 받으며, 프로세스의 code, data, heap영역의 메모리를 공유한다.

</details>

-----------------------

<br>



<br>

-----------------------

### 멀티프로세스와 멀티스레드

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 멀티 프로세스
  - 하나의 프로그램이 여러개의 프로세스로 구성되어 프로그램을 병렬적으로 작업 수행
  - 멀티 프로세스는 하나의 프로세스에 문제가 발생하면 다른 프로세스에 영향을 미치지 않지만 멀티 스레드보다 많은 메모리를 필요로 한다.
- 멀티 스레드
  - 하나의 프로세스에서 여러개의 스레드로 구성되어 자원을 공유하며 작업을 수행
  - 멀티 프로세스보다 적은 메모리를 필요로 하지만 하나의 스레드에 문제가 발생하면 다른 스레드에 영향을 미쳐 프로그램이 종료될수 있다.

</details>

-----------------------

<br>



<br>

-----------------------

### 멀티 프로세스보다 멀티 스레드를 사용하는 이유

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 멀티 스레드는 멀티 프로세스보다 적은 메모리 공간을 차지하고 Context Switching이 빠른 장점이 있다.
- 그리고 공유 메모리를 사용하기 때문에 통신을 위한 system call이 줄어 통신 비용이 줄고 프로세스 생성에 필요한 system call이 줄어 자원을 효율적으로 관리할 수 있다.
- 하지만 동기화 문제나 하나의 스레드 장애로 전체 스레드가 종료될 수 있는 단점이 있다.
- 컨텍스트 스위칭시 stack영역만 초기화 하면 된다.

</details>

-----------------------

<br>



<br>

-----------------------

### 스레드마다 stack을 독립적으로 할당하는 이유

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- stack은 함수 호출시 전달되는 인자, 지역변수, 돌아갈 주소값 등을 저장하기 위한 메모리 공간
- stack을 독립적으로 할당한다는 것은 함수를 독립적으로 실행하는 것이 가능해진다. 이는 독립적인 실행 흐름을 추가하는것이다.
- 즉, 작업을 실행하면서 스레드로 독립적인 실행흐름을 추가하기 위해 스레드에 stack을 독립적으로 할당하는것이다.

</details>

-----------------------

<br>



<br>

-----------------------

### 스레드마다 PC Register를 독립적으로 할당하는 이유

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 스레드의 pc register 에는 명령을 어디까지 수행했는지 저장하는 메모리 공간
- context switching이 발생할 때, 명령 수행 위치를 저장해놔야 다음 cpu에 할당되었을때 명령을 이어서 수행하기 위해 pc register를 독립적으로 할당한다.

</details>

-----------------------

<br>



<br>

-----------------------

### 자바 스레드

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 일반 스레드와 동일하게 프로그램의 흐름 단위.
- JVM에 의해 관리되며 JVM은 프로세스가 없고 스레드만 존재한다.
- 스레드가 생성되면 스레드 정보를 Thread Control Block에 저장하고 JVM에 의해 관리된다.
  - TCB : 커널의 데이터 구조
    - id
    - 스레드 상태
    - 스레드 레지스터 값
    - 스레드가 있는 프로세스의 PCB에 대한 포인터


</details>

-----------------------

<br>



<br>

-----------------------

### 메모리(RAM)

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

<img width="30%" alt="image" src="https://user-images.githubusercontent.com/57162257/182611233-ad4eacd7-2f5b-4957-8146-8e481339fc67.png">

- 메모리 : 프로그램에서 필요로하고 생성되는 값을 저장하는 저장 공간
- 코드 영역 : 실행한 프로그램의 코드 저장 영역
- 데이터 영역 : 전역 변수, 정적 변수 저장 영역
- 힙 영역  : 개발자의 동적으로 할당되는 데이터 저장 영역, 런타임시 크기가 결정된다.
  - 장점: 필요한 데이터의 크기를 알 수 없을 때 사용가능
  - 단점: 힙 경합시 속도 저하 (여러 스레드가 힙 영역의 데이터를 동시에 접근하는 문제)
- 스택 영역 : 지역변수, 매개변수 저장 영역, 컴파일시 크기가 결정된다.
  - 장점: 낭비되는 공간이 없다
  - 단점: 공간을 유연하게 사용할 수 없다.
  - 스택 오버 플로우
    - <img width="500" alt="image" src="https://user-images.githubusercontent.com/57162257/189783086-9f266128-ed46-4155-9253-30b6b1c32faf.png">
    - 재귀 호출이 반복되면 스택 프레임이 계속 쌓이고 스택에 모든 공간을 차지하게 되면 발생되는 에러



</details>

-----------------------

<br>



<br>

-----------------------

### 기억장치

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 데이터, 프로그램, 연산 중간 결과등을 일시적 또는 영구적으로 저장하는 장치
- 레지스터 : CPU에 있는 작지만 빠른 기억장치
- 캐시 : CPU 주기억장치의 접근속도 차이를 해결하기 위한 저장소
- 주 기억장치 : CPU가 직접 읽고 쓸수 있는 장치 (RAM, ROM), 레지스터에 비해 속도가 느리다.
- 보조 기억장치 : CPU가 직접 일고 쓸수 없지만 대용량인 저장소

</details>

-----------------------

<br>



<br>

-----------------------

### 가상 메모리

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 프로세스 전체가 메모리에 올라오지 않고 실행에 필요한 부분만 메모리에 올려서 프로그램을 실행 시킬수 있는 기법
- 가상 메모리가 없다면 프로세스 전체를 메모리에 올려서 사용해야하기 때문에 메모리의 크기에 제약이 많이 따른다.
- 가상 메모리를 사용하게 되면 프로세스에서 필요한 부분만 메모리에 올려서 프로그램을 실행할 수 있기 때문에 메모리의 크기에 제약을 받지 않는다.
- CPU이용률과 처리량이 높아졌다.
- 페이징을 통해 필요한 프로세스를 메모리에 올린다.
- 하지만 물리 메모리로 프로세스를 구동시키는 것보다 느리다.

</details>

-----------------------

<br>



<br>

-----------------------

### 요구 페이징

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- CPU가 프로세스를 실행시킬때 필요한 데이터(페이지)를 메모리에 적재하는 전략
- 가상 메모리는 페이징으로 관리된다.
- 페이지 부재(page fault) 발생시 mmu(memory management unit)에 의해서 인터럽트를 발생시켜 프로세스를 wait상태로 변경 한 후, 요구하는 페이지를 메모리에 올리고 다시 실행한다.

</details>

-----------------------

<br>



<br>

-----------------------

### 메모리 관리 전략

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 제한된 메모리를 효율적으로 사용하고 관리하기 위한 전략
- 문제
  - 내부 단편화 : 메모리공간과 프로세스간의 크기 차이로 공간이 남게 되어 남는 공간을 활용할수 없는것.
  - 외부 단편화 : 프로세스의 크기보다 크게 메모리 공간이 남아있어도 실질적으로 메모리에 프로세스를 할당할 수 없는 것
- 연속 메모리 할당 : 프로세스를 메모리에 연속적으로 할당하는 방법 (외부 단편화 발생)
  - First Fit : 가장 먼저 만나는 빈 메모리 공간에 할당
  - Best Fit : 프로세스 크기와 빈 메모리 공간의 차이가 가장 적은 곳에 할당
  - Worst Fit : 프로세스 크기와 빈 메모리 공간의 차이가 가장 많은 곳에 할당
- 페이징
  - <img width="50%" alt="image" src="https://user-images.githubusercontent.com/57162257/182620581-854c2f6b-5d2f-4096-86c8-6ed4e10c8830.png">
  - 프로세스를 일정 크기인 페이지로 나누고 물리 메모리도 동일한 크기인 프레임으로 나눈다.
  - CPU는 연속적인 논리 메모리로 메모리에 접근하지만 MMU의 페이지 테이블을 사용해서 논리메모리를 물리메모리로 변경하여 나뉘어져있는 프로세스 정보에 접근할 수 있다.
  - 프로세스를 페이지 크기로 나누었을때, 프로세스를 나눈 페이지 중 마지막 페이지에 내부 단편화가 발생한다.
    - 페이지 크기가 1024바이트, 프로세스가 10500바이트일때, 프로세스를 11개의 페이지(11264바이트)로 나누게 되면 마지막 페이지에서 764(1024-260)바이트의 내부 단편화가 발생한다.
    - 페이지를 작게 구성할 수록 내부단편화는 줄어들지만, 페이지를 작게 구성하면 페이지 수가 많아지게 되고 페이지 테이블의 크기가 늘어나 유지 비용이 많이 들게 된다.
  - 장점
    - 메모리에 데이터가 연속적으로 저장될 필요가 없기 때문에 외부 단편화가 발생하지 않는다.
  - 단점
    - 내부 단편화가 발생할 수 있다.
- 세그멘테이션
  - <img width="60%" alt="image" src="https://user-images.githubusercontent.com/57162257/182623165-5c221ef4-bed9-4079-b7bb-3b65a1282c67.png">
  - 페이징과 반대로 프로세스를 고정되지 않는 크기의 세그먼트 단위로 나누어 메모리에 불연속적으로 저장하는 방식.
  - CPU는 논리주소로 메모리에 접근하고 MMU의 세그먼트 테이블을 통해 논리주소를 물리주소로 변경하여 물리 메모리에 저장되어있는 프로세스 정보에 접근할 수 있다.
  - 세그먼트는 논리적 단위이기 때문에 중요도, 용도에 따라 나눌 수 있다. (보호)
    - 페이징은 동일한 크기로 나뉘어져있기 때문에 code,data,stack 구분없이 페이징될 수 있다.
    - 세그먼트는 부위별로 자를수 있기 때문에 섞이지 않는다. (code, data, stack 끼리)
    - 세그먼트 테이블의 세그먼트마다 r,w,x 비트(권한)를 부여할 수 있다.
      - r 읽기
      - w 쓰기
      - x 실행
  - 메모리 낭비 방지 (공유)
    - 같은 프로그램을 사용하는 복수개의 프로세스가 있는 경우, 세그먼트 테이블이 code영역을 같은 곳을 가리키게 만들수 있다.
    - 페이징은 영역별로 명확하게 나눌 수 없기 때문에 그럴 수 없다.
  - 장점 : 보호, 공유
  - 단점 : 외부 단편화가 발생할 수 있다.
    - 세그먼트는 가변의 크기이기 때문에 메모리 낭비가 발생할 수 있다.
- 페이징 또는 세그멘테이션을 사용하는 이유
  - 디스크에서 메모리로 데이터를 적재하는 과정에 단편화가 발생한다. 연속 메모리 할당을 통해 단편화를 해결할 수 있지만, 메모리 계산 비용이 적은 페이징 또는 세그멘테이션을 주로 사용한다.
  - 세그먼트는 보호, 공유 면에서 효과적이고 페이징은 외부 단편화 문제를 해결해준다. 그렇기 때문에 세그먼트를 페이징하는 방법을 사용한다.
    - 두번의 MMU를 거치기 때문에 속도면에서 떨어진다는 단점이 있다.


</details>

-----------------------

<br>



<br>

-----------------------

### 페이지 교체 알고리즘

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 가상 메모리를 관리하는 페이징 기법에서 페이지를 교체할때 사용하는 알고리즘
- 종류
  - FIFO (First In First Out) : 가장 먼저 들어온 페이지가 가장 먼저 교체된다.
  - LRU (Least Recently Use) : 최근에 가장 오랫동안 사용하지 않은 페이지가 교체된다. (가장 많이 사용)
    - LRU를 사용하게 되면 각 페이지의 사용빈도를 체크해야 하기 때문에 큰 오버헤드 발생 
  - LFU (Least Frequently Use) : 최근에 사용빈도가 가장 적은 페이지가 교체된다.
    - 사용횟수를 증가시키 때문에 오버헤드 발생
  - OPT (Optimal) : 앞으로 가장 오랫동안 사용하지 않을 페이지를 예상해서 교체된다. (연구목적)
  - NRU (Not Recently Use) : LRU와 동일하게 최근에 사용한지 오래된 페이지를 교체하는데, 참조비트와 변형 비트를 사용해서 LRU의 오버헤드를 줄인 알고리즘
    - 참조 비트 : 페이지에 참조했다면 1, 참조하지 않았다면 0
    - 변형 비트 : 페이지 내부 내용이 변경되었다면 1, 변경되지 않았다면 0
    - 참조 비트의 0이 변형비트의 0보다 교체 우선순위가 높다.

</details>

-----------------------

<br>



<br>

-----------------------

### Trashing

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 페이지 부재가 빈번하게 발생하면, 프로세스 실행 시간보다 페이지 교체 시간이 더 많은 상태
- 해결방법
  - 다중 프로그래밍 빈도를 줄인다.
  - Working Set 알고리즘
    - 특정 기간동안 사용되는 페이지 개수를 파악해서 해당 페이지 개수만큼의 프레임이 확보된다면 페이지를 메모리에 올리는 알고리즘
  - Page Fault Frequency 알고리즘
    - page fault 퍼센트의 상한과 하한을 지정한 후, 상한을 넘게된다면 주어지는 프레임의 개수를 늘려주고 하한을 넘게된다면 주어지는 프레임의 개수를 줄이는 알고리즘

</details>

-----------------------

<br>



<br>

-----------------------

### 프로세스 상태

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- <img width="500" alt="image" src="https://user-images.githubusercontent.com/57162257/193495820-06a82f20-7d8f-41af-aa88-955971e9705c.png">
- 생성 : 프로세스가 생성되는 상태
- 준비 : 프로세스가 CPU에 실행되고 있는 상태는 아니지만 언제든지 CPU에 할당될수 있는 상태
- 실행 : CPU에 할당되어 작업을 수행하는 상태
- 대기 : 입출력과 같은 인터럽트에 의해서 프로세스가 대기중인 상태
- 종료 : 프로세스의 작업이 끝나 종료된 상태

</details>

-----------------------

<br>



<br>

-----------------------

### 스케줄러

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 프로세스를 스케줄링 하기 위해 3개의 큐가 존재한다
  - Job Queue : 존재하는 모든 프로세스를 담은 큐
  - Ready Queue : CPU할당을 대기하는 프로세스를 담은 큐
  - Device Queue : Device IO 출력을 위해 대기하고 있는 프로세스를 담은 큐

- 이 큐를 관리해주기 위해 3개의 스케줄러가 존재한다.
- 장기 스케줄러 : 디스크 내의 프로세스를 메모리에 가져올지 결정하여 프로세스를 준비 상태로 보내는 스케줄러
- 단기 스케줄러 : Ready Queue에 존재하는 프로세스 중 CPU에 할당될 프로세스를 정하는 스케줄러
- 중기 스케줄러 : CPU에 할당되기 위해 너무 많은 프로세스가 몰리게 되면 메모리를 관리하기 위해 프로세스를 디바이스 영역으로 보내는 스케줄러
  - Swap out : 디바이스 영역으로 보냄
  - swap in : 메모리로 다시 들임

</details>

-----------------------

<br>



<br>

-----------------------

### 인터럽트

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- CPU가 프로세스를 처리하고 있을 때, 입출력이나 예외가 발생하면 해당 이슈를 처리하기 위해 CPU에게 알려주는 것
- 종류
  - 외부 인터럽트 : CPU가 아닌 외부에서 CPU에게 어떤 사실을 알려주는것
    - 타임아웃, I/O
  - 내부 인터럽트 : CPU내부에서 잘못된 명령이나 데이터로 인해 발생한 문제나 이벤트를 알려주는 것
    - 예외
  - 시스템 인터럽트 : 감시 프로그램 호출
- 우선순위
  - 주로 외부 인터럽트를 우선적으로 처리한다
  - 전원 이상 > 타임아웃 > 입출력 > 예외 > 감시 프로그램

- 우선순위 판별
  - 소프트웨어 방법
    - 내부적으로 인터럽트 요청을 확인하여 우선순위가 높은 인터럽트를 찾는다.
    - 우선순위 변동이 쉽고 간단하다. 하지만 하드웨어 방법에 비해 처리가 느리다.

  - 하드웨어
    - 인터럽트 요청 장치와 CPU사이의 장치식별 번호를 통해 높은 우선순위의 인터럽트를 찾는다.
    - 복잡하며 소프트웨어보다 비효율적이지만, 처리속도가 빠르다.

- 인터럽트 수행 과정
  1. CPU 실행 중 인터럽트 발생
  2. 수행중인 프로세스를 멈추고 PCB에 상태를 저장하고 pc에 다음 수행할 작업을 저장한다.
  3. 인터럽트 백터를 읽어 해당 인터럽트의 인터럽트 서비스 루틴(인터럽트 핸들러)으로 이동
     1. 인터럽트 백터
        1. 인터럽트 발생시 처리해야할 인터럽트 서비스 루틴의 주소를 인터럽트 별로 보관하고 있는 테이블
  4. 인터럽트 서비스 루틴 (인터럽트 핸들러)를 통해 루틴을 실행하여 문제를 해결한다.
     1. 인터럽트 서비스 루틴에는 특정 인터럽트 발생시 처리해야할 내용이 이미 프로그램되어 있기 때문에 인터럽트 백터를 이용해 인터럽트 서비스 루틴의 주소를 알아내고 해당 인터럽트에 대해 수행해야할 프로그램을 수행하여 문제를 해결한다.
  5. 완료 후, 인터럽트를 해제하고 저장된 프로세스 정보를 가져와 다시 수행.

</details>

-----------------------

<br>



<br>

-----------------------

### CPU 스케줄링

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 대기하고 있는 프로세스를 CPU에 할당시키는 스케줄링으로써, 단기 스케줄러에 의해 수행되며 CPU의 선점 여부에 따라서 선점 스케줄링과 비선점 스케줄링으로 나뉜다.
- 종류
  - 선점 스케줄링 : CPU의 제어권을 뺏음
  - 비선점 스케줄링 : CPU의 제어권을 뺏지 않음
- 비선점 스케줄링
  - FCFS (First Come First Service) : CPU를 요청한 순서대로 CPU에 할당
  - SJF (Short Job First) : CPU 요구량이 가장 적은 프로세스부터 할당
  - HRN (Highest Response-Ratio Next) : 우선순위를 계산하여 점유 불평등을 보완하는 방법
    - 우선순위 = (대기시간 + 실행시간)/실행시간
- 선점 스케줄링
  - 라운드 로빈 (Round Robine) : FCFS처럼 CPU를 요청한 순서대로 할당하지만, 각 프로세스마다 할당가능한 시간을 주고 선점하는것.
  - SRT (Shortest Remaning Time) : SJF처럼 CPU요구량이 적은 프로세스부터 할당하지만, 중간에 더 적은 프로세스가 있다면 선점하는것.
  - 다단계 큐 (Multi-level Queue) 
    - 각 큐마다 절대적인 우선순위와 스케줄링을 적용하며 프로세스는 종류와 우선순위에 따라 큐에 배치된다.
    - 각 큐마다 프로세스는 이동할 수 없기 때문에 스케줄링 부담은 적지만 유연함이 부족하다.
    - 내부적으로 RR을 사용하며 우선순위가 가장 낮은 큐는 FCFS를 사용한다.
  - 다단계 피드백 큐 (Multi-level Feedback Queue)
    - 각 큐마다 절대적인 우선순위와 스케줄링을 적용한다.
    - 하지만 각 큐마다 프로세스가 이동할 수 있기 때문에 기아현상을 방지하고 유연하게 CPU에 할당할수 있다.
    - 내부적으로 RR을 사용하며 주어진 시간을 사용했는데 아직 bursttime이 남아있다면 우선순위가 낮은 큐로 이동하여 더 많은 시간을 할당받는다. 우선순위가 가장낮은 큐는 FCFS를 수행한다.

</details>

-----------------------

<br>



<br>

-----------------------

### 커널

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 기본적으로 소프트웨어는 메모리에 올라가있어야지 컴퓨터 시스템상에서 작동할 수 있다.
- 운영체제도 소프트웨어이기 때문에 메모리에 올라가있어야 실행되는데, 모든 운영체제를 메모리에 올리게 되면 메모리 낭비가 심하기 때문에 운영체제 중 항상 필요한 부분만 메모리에 올라가 있는데 이를 커널이라고 한다.
- 즉, 메모리에 상주하고 있는 운영체제의 핵심 부분

</details>

-----------------------

<br>



<br>

-----------------------

### System Call

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 운영체제는 기능을 제한하여 보안을 하기 위해 커널 모드와 사용자 모드로 나누어 구동한다.
- 시스템 콜은 커널 모드의 기능을 사용자 모드에서 사용하기 위해 프로세스가 운영체제에게 요청하는 인터페이스.
- 분류
  - 프로세스 제어
    - fork
      - 프로세스의 생성과 제어를 위한 시스템 콜
      - 새로운 프로세스를 만드는 것
      - PCB/프로세스 메모리 구조가 복사된다.
        - 새로운 프로세스는 원래 프로세스와 똑같은 코드를 가지고 있다.
    - exec
      - 프로세스의 생성과 제어를 위한 시스템 콜
      - 프로세스를 새로운것으로 덮어씌워 실행시키는 것
      - 새로운 메모리를 할당하지 않고 현재 프로세스를 덮어 씌워 실행
        - 새로운 프로세스를 생성하는 것이 아닌 기존 PID를 사용하는 새로운 프로세스로 사용되는것.
  - 파일 조작
  - 장치 관리
  - 정보 유지
  - 통신

</details>

-----------------------

<br>



<br>

-----------------------

### PCB (Process Control Block)

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- PCB는 프로세스의 정보인 Process Metadata를 저장하는 곳
- OS스케줄러에 의해 context switching되는 정보 단위
- 커널내에 존재
- Process Meta Data
  - Process ID
  - Process State
  - Process Priority
  - Program Count : 다음에 실행할 명령어 주소

</details>

-----------------------

<br>



<br>

-----------------------

### TCB(Thread Control Block)

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />




-----------------------

- 스레드 라이브러리에 의해 context swtiching되는 정보단위
- Thread Meta Data
  - Thread ID
  - Thread Status
  - PC
  - Register
  - Process ID


</details>

-----------------------

<br>



### PCB 필요한 이유

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- context switching시 실행중인 프로세스의 정보를 저장하고 다음 프로세스를 실행시킬 때 PCB를 이용해 프로세스의 정보를 저장하고 불러온다.

</details>

-----------------------

<br>



### PCB 저장과정

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

1. 프로그램 실행
2. 프로세스 생성
3. 프로세스 메모리 (code, data, stack) 공간 할당
4. 프로세스의 메타데이터가 커널의 PCB에 저장

</details>

-----------------------

<br>



<br>

-----------------------

### Context Switching

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 스케줄링에 따라 실행중인 프로세스의 정보를 저장하고 새로운 프로세스의 정보를 CPU에 넘겨주는 작업 과정
- CPU가 교체되는 프로세스의 정보를 PCB에 저장하고, 새로운 프로세의 정보를 PCB에서 불러와 CPU의 레지스터에 적재
- 인터럽트가 발생하면 인터럽트 처리를 수행하기 전에 발생
- 프로세스
  - 프로세스의 context switching은 실행중이던 프로세스의 정보를 PCB에 저장하고 캐시를 지운 다음에 CPU에 할당시킬 프로세스의 정보를 PCB에서 가져와 CPU에 적재시켜야한다.

- 스레드
  - 스레드의 context switching은 실행중이던 스레드의 정보를 TCB에 저장해한다. 하지만 프로세스 context switching과는 다르게 메모리를 공유하고 있어 초기화 비용을 줄여 빠르게 context switching이 발생한다.


</details>

-----------------------

<br>



<br>

-----------------------

### Context Switching 발생 과 비 발생

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 발생
  - Dispacher에 의해서 Ready Queue의 프로세스가 CPU에 할당될 때
  - Time 인터럽트 발생 시
  - 입출력 요청 system call 발생 시
- 비발생
  - Time 인터럽트를 제외한 인터럽트
  - 입출력 요청 system call을 제외한 system call
  - 두 경우시 모두 운영체제가 커널모드로 변경되어 실행될 뿐 실행중인 프로세스를 중단했다가 요청을 처리한 후 다시 실행한다.

</details>

-----------------------

<br>



<br>

-----------------------

### Context Switching 오버헤드

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- context swtiching시 새로운 프로세스 정보를 CPU의 레지스터에 적재할 때 CPU는 아무런 작업을 할 수 없다.
- 이 때를 context switching의 오버헤드라고 한다.
- 해결 방안
  - 다중 프로그래밍 수준을 낮추어 context switching 발생 빈도를 줄인다.
  - 스레드를 이용해서 context switching 부하를 최소화한다.
    - 스레드는 프로세스와 다르게 공유 메모리 공간이 있기 때문에 context switching이 발생해도 캐시 데이터를 공유 자원에 저장하고 있고 메모리 주소를 변경하지 않아도 되기 때문에 비용적으로나 시간적으로나 효율적이게됩니다.

</details>

-----------------------

<br>



<br>

-----------------------

### Dispatcher

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 스케줄러가 Ready 상태의 프로세스 중 하나를 CPU에 할당시켜 Run 상태로 변경하는 요소를 Dispatcher라고 한다.
- dispatcher가 Ready 상태의 프로세를 CPU에 할당하는데 까지 걸리는 시간을 Dispatcher Latency라고 한다.
- Ready상태의 프로세를 Run상태로 변경하는 것을 Dispatcher가 수행하고 실행중인 프로세스의 정보를 PCB에 저장하고 새로운 프로세스의 정보를 PCB에서 가져와 실행시키는 모든과정을 Context Swtiching이라고 한다.
- 즉, Context Switching과정에 Dispatch 과정이 포함된 구조이다.

</details>

-----------------------

<br>



<br>

-----------------------

### Synchronization (동기화)

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 프로세스 동기화 : 하나의 자원을 동시에 여러 프로세스가 동시에 접근하는것을 제어
- 스레드 동기화 : 하나의 코드블럭 또는 메소드를 여러 스레드가 동시에 접근하는 것을 제어
- 동기화 방법 : 뮤텍스, 세마포어

</details>

-----------------------

<br>



<br>

-----------------------

### 뮤텍스 & 세마포어

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 뮤텍스와 세마포어는 여러개의 프로세스나 스레드가 공유자원 또는 임계영역(critical section)에 동시에 접근하는 것을 막기위한 방법
- 뮤텍스 (Mutex) : 오직 한개의 프로세스나 스레드 만이 공유자원에 접근하도록 제어하는 것. 동기화 대상이 하나이다.
- 세마포어 (Semaphore) : 세마포어 변수만큼의 스레드나 프로세스가 접근할수 있도록 제어하는 것. 동기화 대상이 여럿일 수 있다.

</details>

-----------------------

<br>



<br>

-----------------------

### 교착상태(Dead Lock)

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 두개 이상의 프로세스나 스레드가 자원을 점유한 상태에서 서로의 자원을 점유하기 위해 기다려서 무한한 대기가 발생하는 것.
- 발생 조건
  - 상호 배제 (Mutual Exclusion) : 하나의 자원에 여러 프로세스의 동시접근을 막는것.
  - 점유와 대기 (Hold and Wait) : 하나의 자원을 점유하고 다른 프로세스의 자원을 요청하는 것.
  - 비선점 (Non Primitive) : 점유하고 있는 자원의 제어권을 뺏을수 없는 것
  - 환영 대기 (Circle Wait) : 각 프로세스가 다음 프로세스가 요청하고 있는 자원을 점유하는 것
- 해결방법
  - 예방 (Prevention) : 4가지의 발생 조건 중 하나를 해결하여 예방하는 것.
  - 회피 (Avoidance) : 발생하지 않도록 알고리즘을 설계하여 교착상태를 피하는 것.
    - 은행원 알고리즘 : 자원을 할당한 후에도 안정 상태로 유지되는지 사전에 검사하여 교착상태를 회피하는 것.
  - 탐색 (Detection) : 자원할당 그래프를 모니터링 해서 교착상태를 탐지하는 것
  - 회복 (Recovery) : 교착 상태가 탐지되면 복구를 실행.

</details>

-----------------------

<br>



<br>

-----------------------

### IPC (Inter Process Communication)

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 독립적인 프로세스간 통신하기 위한 설비
- 프로세스는 커널에서 제공해주는 IPC 설비를 통해 프로세스간 통신이 가능하다.
- IPC에서는 프로세스간 데이터를 동기화하고 보호하기 위해서 뮤텍스와 세마포어를 사용한다.
- 종류
  - 익명 Pipe
    - 두개의 프로세스를 연결하여 한쪽에서는 쓰기만, 다른 한쪽에서는 읽기만 가능한 반이중 통신.
    - 전이중 통신을 위해서는 2개의 파이프가 필요하다.
    - 통신할 프로세스간의 관계가 명확한 상태에서만 사용할 수 있다. (부모-자식 프로세스 관계)
  - Named Pipe
    - 익명 Pipe와 마찬가지로 반이중 통신.
    - 전이중 통신을 위해서는 2개의 파이프가 필요하다.
    - 두 프로세스간 관계가 명확하지 않아도 통신이 가능하다.
  - Message Queue
    - Pipe는 데이터의 흐름이고 Message Queue는 메모리 공간을 가진다.
    - 메세지에 데이터를 담은 후 번호를 남기고 전역으로 관리되는 하나의 큐를 통해 여러 프로세스가 동시에 데이터를 주고 받을수 있다.
  - Shared Memory
    - Pipe와 Message Queue는 통신을 위한 설비라면, Shared Memory는 데이터 자체를 공유하기 위한 설비이다.
    - 프로세스간 메모리 영역을 공유해서 같이 사용할 수 있도록 허용한다.
    - 중계자 없이 데이터를 공유하기 때문에 IPC 중 가장 빠르다.
  - Socket
    - 네트워크 소켓 통신을 통해 데이터 공유
    - 네트워크상에서 호스트칸 데이터를 전달하기 위해 소켓을 사용한다.
      - 소켓 : 네트워크상에서 데이터를 내보내거나 받기위한 실제적 창구역할
        - 프로토콜(TCP,UDP) + IP + 포트 로 정의할 수 있다.
      - 포트 : 네트워크상에서 통신을 위해 호스트 내부 프로세스가 할당받는 식별자

</details>

-----------------------

<br>



<br>

-----------------------

### Race Condition & Critical Section

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- Race Condition
  - 하나의 자원에 대해 여러 프로세스/스레드가 접근하였을때, 예상한 결과를 보장할수 없는 경우
- Synchronization
  - 여러 프로세스/스레드가 공통 자원에 접근하여도 일관성있는 데이터를 유지하는 것

- Critical Section (임계 영역)
  - 일관성있는 데이터를 제공하기 위해 공통자원에 하나의 프로세스/스레드의 접근을 허용하여 실행하도록 하는 영역
- 임계 영역을 문제를 해결하기 위한 조건
  - 상호 배제 (Mutal Exclusion) : 하나의 프로세스가 임계영역에 있다면 다른 프로세스의 접근을 막는것.
  - 진행 (Progress) : 임계영역을 사용하는 프로세스가 없을때 프로세스의 접근이 가능하다
  - 한정 대기 (Bounded Waiting) : 임계영역에 들어갔다온 프로세스는 다음 접근에 제한을 받음.

</details>

-----------------------

<br>



<br>

-----------------------

### CPU 성능 척도

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- CPU Utilization (이용률) : CPU가 쉬지않고 일한 시간
- Thorughput (처리량) : 단위 시간 처리량
- Turnaround Time (소요시간) : 프로세스의 CPU사용시간 + 대기 시간
- Waiting Time (대기시간) : 프로세스가 Ready Queue에서 대기한 총 시간
- Response Time (응답시간) : 프로세스가 최초 Ready Queue에 들어가서 CPU에 할당되기까지 걸린 시간

</details>

-----------------------

<br>



<br>

-----------------------

### 캐시

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- CPU가 주기억장치에 접근하는 횟수를 줄여주기 위해 주기억장치의 특정 정보를 저장하는 저장소로 캐시 지역성을 통해 주기억장치의 특정 정보를 캐시에 저장한다.
- 캐시 적중률 (Hit)
  - CPU가 필요로하는 데이터를 캐시가 가지고 있다면 더 빠르게 데이터를 가져올 수 있다.
  - 캐시 적중률 = 캐시 메모리 적중 횟수 / 전체 메모리 참조 횟수
- 메모리 접근 순서
  - 캐시 -> 주기억장치 -> 보조 기억장치
- 캐시 종류
  - L1
    - 고성능이지만 고가인 작은 용량의 캐시, 명령어 캐시와 데이터 캐시로 나누어져 있다.
      - 명령어 캐시 (I Cache) : 메모리의 텍스트영역을 다루는 캐시 (공간지역성이 높다)
      - 데이터 캐시 (D Cache) : CPU가 직전에 사용한 데이터나 바로 실행해야하는 중요한 파일 저장 (시간지역성이 높다)

    - 두 캐시로 나뉘어져있기 때문에 서로 다른 지역성을 이용할 수 있고 명령어와 데이터를 동시에 읽어올수 있어 CPU의 파이프라이닝 성능을 향상시킬 수 있다.

  - L2
    - L1캐시에서 데이터를 가져오기 위한 캐시로, L1보다 용량이 크고 별도의 캐시로 나뉘어지지 않는다.

  - L3
    - 멀티코어 시스템에서 코어들이 공유하는 캐시 메모리

  - 각 캐시는 CPU내부에 존재한다.
- 캐시 내부구조
  - <img width="450" alt="image" src="https://user-images.githubusercontent.com/57162257/194767407-9d1cf78b-6743-4dd5-8188-8a5c071793b6.png">
  - S개의 집합으로 이루어져있고 각 집합은 E개의 캐시 라인을 저장하고 있다.
  - 하나의 캐시라인은 주기억장치에서 가져오는 하나의 블럭이다.
    - 유효성을 나타내는 v
    - 동일 집합에서 블럭을 나타내는 tag
    - B크기의 블럭

  - Cache Size = v + tag + B (byte)
- 캐시 정책
  - 주기억장치의 데이터 블럭은 캐시 메모리의 특정 영역에만 저장될 수 있다. (`해시`)
  - 예를 들어 주기억장치의 2번째 블럭(0001)은 캐시의 두번째 집합(001)에만 저장될 수 있다. 그 이유는, 주기억장치의 블럭이 캐시의 어느 영역에나 저장되게 된다면, CPU에서 특정 블럭을 요청할때 캐시에서 해당 블럭이 있는지 확인하기 위해 선형탐색으로 모든 캐시 메모리를 확인해야한다.O(S*E)
    하지만 특정 집합에만 저장될 수 있게 된다면 해당 집합에 대해서만 찾으면 되기 때문에 더 빠르게 찾을 수 있게 된다. O(E)
- 캐시 읽기 동작
  - <img width="200" alt="image" src="https://user-images.githubusercontent.com/57162257/194767800-8eaa5e13-78d0-48ec-82d7-368a30cb87d9.png">
  - 과정
    1. CPU가 참조하고 싶은 메모리 주소를 전달한다.
    2. 캐시는 메모리 주소의 s bits를 통해 캐시의 해당 집합으로 이동한다.
    3. 메모리 주소의 t bits와 캐시라인의 tag들을 비교한다.
    4. 캐시 라인의 v bits를 통해 유효성 검사를 확인하고 유효하고 tag를 찾게 되면 HIT
  - 캐시 히트인 경우 메모리 주소에서 요청하는 b bits를 통해 해당 블럭에서 블럭을 전달.
  - 캐시 미스인 경우 주기억장치에서 해당 블럭을 불러와 캐시에 저장하고 CPU에 전달.
    - 만약 캐시에 저장공간이 부족하게 되면 FIFO, LFU, LRU 알고리즘을 통해 블럭을 교체한다.
- 캐시 쓰기 동작
  - 수정하려는 블럭이 히트인 경우
    - write through
      - 캐시의 내용을 수정하고 주기억장치에도 동기화하여 캐시와 주기억장치의 동시성 보장
        - 하지만 주기억장치의 데이터를 update하기 까지 대기 시간이 발생하여 성능이 떨어진다.

    - write back
      - 캐시의 내용을 수정하고 해당 캐시 블럭을 교체할때 주기억장치 블럭의 내용을 동기화한다.
      - 주기억장치의 접근을 최소화하기때문에 성능은 향상되지만 주기억장치 블럭의 수정이 필요한지 여부를 알려줄 추가 bit가 필요하고 캐시와 주기억장치간의 일관성 불일치 문제가 발생할 수 있다.
  - 수정하려는 블럭이 미스인 경우 (allocate - 할당)
    - write allocate
      - 주기억장치 블럭의 내용을 수정한 뒤 캐시로 가져오는 방식
  
    - no-write allocate
      - 주기억장치 블럭만 수정하고 캐시에 올리지 않는 방식
- 캐시의 성능
  - 평균접근시간 = Hit latency + Miss Rate * Miss latency
    - 캐시의 크기를 줄여 Hit latency를 줄이거나 캐시 크기를 늘여 Miss Rate를 줄여 캐시의 성능을 향상시킬수 있다.
- Associatvie Cache (연관 캐시) = 캐시 구조
  - CPU가 메모리 주소를 요청했을때 서로 같은 캐시메모리 주소이지만 다른 주기억장치 주소라면 충돌이 발생한다.
  - Direct Mapped Cache
    - 주기억장치의 여러 주소가 캐시 메모리의 한 주소에 대응되는 1:n 방식
    - <img width="200" alt="image" src="https://user-images.githubusercontent.com/57162257/195005471-06ab5bb7-e253-46f9-89b0-3c6e83b634e1.png">
    
  - Fully Associative Cache
    - 비어있는 캐시 공간이 있다면 아무 메모리 주소를 저장할 수 있다.
    - 모든 캐시 메모리를 탐색해야하기 때문에 검색 속도는 느리지만 저장은 빠르다.
    
  - Set Associative Cache
    - 하나의 캐시 라인(캐시 주소)에 2개 이상의 메인 메모리 주소를 할당하여 DMC보다 검색은 느리지만 저장은 빠르고 Fully Associatee Cache보다는 저장은 느리지만 검색은 빠르다.
    - <img width="200" alt="image" src="https://user-images.githubusercontent.com/57162257/195005499-6bf835a6-6da0-4a9e-aa28-addc8f18129b.png">


</details>

-----------------------

<br>



<br>

-----------------------

### 캐시 지역성

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 캐시는 사용 예상되는 데이터를 저장하여 CPU의 처리속도를 빠르게 도와준다.
- CPU가 캐시를 참조했을때 쓸모있는 정보에 따라 처리속도가 달라진다.
- 적중률(Hit rate)를 극대화시키기 위해 데이터의 지역성을 사용한다.
- 지역성
  - 시간 지역성 : 최근에 참조된 데이터는 곧 다시 참조되는 특징
  - 공간 지역성 : 참조된 데이터의 인근 데이터가 참조되는 특징
- 지역성을 이용해 데이터를 캐시에 담고있다면 적중률을 높일수 있다.

</details>

-----------------------

<br>



<br>

-----------------------

### 기아현상 & 에이징 기법

<details>
   <summary> 예비 답안 보기 (👈 Click)</summary>
<br />



-----------------------

- 기아현상 : 프로세스의 할당 우선순위가 낮아 지속적으로 CPU의 할당을 받지 못하는 문제
- 에이징 기법 : 기아현상을 해결하기 위한 방법으로 프로세스의 할당 우선순위를 대기시간에 비례하여 점진적으로 높여주어 CPU에 할당될수 있도록 하는 방법

</details>

-----------------------

<br>